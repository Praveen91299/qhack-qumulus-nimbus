{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QHack Open Project  \n",
    "## Team Qumulus Nimbus  \n",
    "Team members: Praveen J and my cats. PS, I'm not a cat. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project abstract:  \n",
    "We provide a pennylane implementation of single qubit universal quantum classifier similar to that presented in [1] and [2]. We then address quantum classifiers by data reuploading for **Quantum Data** and show it's performance, which we believe has not been done before. We then address the same with multiple qubit quantum classifier attempting to prove our hypothesis that multiple qubits may not help. Additionally we propose systematic and physically meaningful encoding gates for higher dimensional classical data and show it's performance on test data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:  \n",
    "\n",
    "\n",
    "Since our goal as a qhack project is to showcase our new ideas of encoding and processing methods, we restrict all our methods to binary classification, but with some extra short steps, all can be extended to mulitple class classification by methods similar to that outlined in [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4, 0.2, 0.1], [0.4, 0.2, 0.1]]\n",
      "Cost at step 0: 0.4798480457101114\n"
     ]
    }
   ],
   "source": [
    "#simple re-implementation of classical data uploading\n",
    "\n",
    "#initializations\n",
    "num_qubit = 1\n",
    "num_layers = 5\n",
    "X_train = [[0.5, 0.3, -0.4], [0.4, 0.2, 0.1]]\n",
    "Y_train = [-1, 1]\n",
    "weights = np.random.uniform(low=-np.pi / 2, high=np.pi / 2, size=(num_layers, 9)) #assuming 3D data input\n",
    "batch_size = 2\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubit)\n",
    "opt = qml.GradientDescentOptimizer(stepsize=0.2)\n",
    "\n",
    "def apply_data_layer(params, wires, data):\n",
    "    qml.Rot(params[0, 0]*data[0] + params[0, 1], \n",
    "            params[1, 0]*data[1] + params[1, 1], \n",
    "            params[2, 0]*data[2] + params[2, 1], \n",
    "            wires=wires)\n",
    "\n",
    "def apply_mixing_layer(params, wires):\n",
    "    qml.Rot(params[0], \n",
    "            params[1],\n",
    "            params[2],\n",
    "            wires=wires)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def apply_layers(params, wires, data):\n",
    "    #reshaping parameters for ease\n",
    "    data_params = params[:,:6].reshape((len(params), 3, 2))\n",
    "    mixing_params = params[:,6:].reshape((len(params), 3))\n",
    "    for i in range(num_layers):\n",
    "        apply_data_layer(data_params[i], wires=wires, data=data)\n",
    "        apply_mixing_layer(mixing_params[i], wires=wires)\n",
    "    return qml.expval(qml.PauliZ(wires))\n",
    "\n",
    "def square_loss(labels, predictions): #just defining it, but not using it\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def fidelity_loss(labels, predictions): #fidelity loss, unweighted\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (1 - l*p)/2\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def cost(w, X, Y):\n",
    "    predictions = [apply_layers(w, wires=0, data = x) for x in X]\n",
    "    return fidelity_loss(Y, predictions)\n",
    "\n",
    "n = int(len(data)/batch_size)\n",
    "\n",
    "for a in range(n):\n",
    "    batch_index = np.random.randint(0, len(X_train), (batch_size,))\n",
    "    X_train_batch = []\n",
    "    Y_train_batch = []\n",
    "    for i in batch_index:\n",
    "        X_train_batch.append(X_train[i])\n",
    "        Y_train_batch.append(Y_train[i])\n",
    "    weights = opt.step(lambda w: cost(w, X_train_batch, Y_train_batch), weights)\n",
    "    print(X_train_batch)\n",
    "    print('Cost at step '+ str(a) + ': ' + str(cost(weights, X_train_batch, Y_train_batch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data classification and re-uploading with a qram  \n",
    "We use a simple amplitude encoding as described in [3] to encode 2 dimensional and 3 dimentional input data. We restrict ourselves to binary classification for simplicity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_2D(data, wires):\n",
    "    qml.Rot(data[0], data[1], 0, wires=wires)\n",
    "\n",
    "def encode_3D(data, wires):\n",
    "    qml.Rot(data[0], data[1], data[2], wires=wires)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each encoding step, we use controlled gates, controlled on the qram qubits. This allows us to parallely compute multiple training cases, while trading off circuit depth. Since we are using circuit fidelity as given in the equation above, we do something smart, we apply controlled gates again based on the targetted training classification \\[0, 1\\] and then we measure the output expectation value (perform a state tomography for fidelity). We apply a NOT gate if the label is -1 ($|1\\rangle\\langle1|$). On measuring expectation over Pauli Z basis, we get:\n",
    "State post circuit:\n",
    "\n",
    "\\begin{align*}\n",
    "    |\\psi\\rangle &= \\frac{1}{\\sqrt{N}} \\sum_i |i\\rangle|\\psi_i\\rangle \\text{ where } \\psi_i \\text{ refers to state post circuit} \\\\\n",
    "    x &= \\frac{1}{N}(\\sum_i \\langle \\psi_i | (|0\\rangle\\langle 0| - |1\\rangle\\langle 1|)|\\psi_i \\rangle)\\\\\n",
    "    \\text{Required fidelity } f &= \\frac{1}{N}(\\sum_i \\langle \\psi_i | (|0\\rangle\\langle 0|)|\\psi_i \\rangle)\\\\\n",
    "    &= \\frac{1}{2N}(\\sum_i \\langle \\psi_i |(|0\\rangle\\langle 0| + |1\\rangle\\langle 1| + |0\\rangle\\langle 0| - |1\\rangle\\langle 1|)|\\psi_i \\rangle)\\\\\n",
    "    &= \\frac{1}{2}(1 + x)\\\\\n",
    "    \\text{Cost function: } L &= 1 - f = \\frac{1}{2}(1 - x) \\text{ where x is the measured value.}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "For a n qubit qram, we can process a batch size of $2^n$. Additionally we can proces batches of size $m \\neq 2^k$ by not performing gate operations on those states and appropriately modifying fidelity in the end. For a batch size of m, qrammed over n qubits, $N = 2^n > m$:\n",
    "$$\\text{Cost function: } L = \\frac{1}{2}(1-x)\\times \\frac{N}{m}$$\n",
    "\n",
    "Additionally to optimise the circuit, we perform some preprocessing on the rotation angles to reduce some mulitple qubit controlled rotation gates into uncontrolled and lower order control rotation gates. For example, for a qram over states (batch size 4), if $a, b, c, d$ were the rotation angles for each of them, we have the following decomposition:\n",
    "\n",
    "Now let's implement this for a batch size of 4 (2 qubit qram). Note that for ease of implementation, we have introduced a 4th qubit to act as an indicator for $|11\\rangle$ qram state so as to avoid repeated Toffoli gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5, 0.3, 0.6], [0.5, 0.3, -0.4], [0.4, 0.2, 0.1], [-0.5, 0.3, 0.6]]\n",
      "Cost at step 0: 0.4506197432768578 cal in 0.08333683013916016\n",
      "0.17219777959433907 in 0.0886385440826416\n"
     ]
    }
   ],
   "source": [
    "#initialization\n",
    "num_qubit = 4\n",
    "layers = 5\n",
    "X_train = [[0.5, 0.3, -0.4], [0.4, 0.2, 0.1], [0.4, 0.2, 0.2], [-0.5, 0.3, 0.6]]\n",
    "Y_train = [-1, 1, 1, -1]\n",
    "batch_size = 4 #qrammed!\n",
    "weights = np.random.uniform(low=-np.pi / 2, high=np.pi / 2, size=(num_layers, 9)) #3D input\n",
    "\n",
    "dev2 = qml.device(\"default.qubit\", wires=num_qubit)\n",
    "opt = qml.GradientDescentOptimizer(stepsize=0.2)\n",
    "\n",
    "def apply_data_layer_qram(params, wires, data):\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    for i in range(len(data)):\n",
    "        a.append(params[0, 0]*data[i][0] + params[0, 1])\n",
    "        b.append(params[1, 0]*data[i][1] + params[1, 1])\n",
    "        c.append(params[2, 0]*data[i][2] + params[2, 1])\n",
    "    transform = lambda arr: [arr[0], arr[1] - arr[0], arr[2] - arr[0], arr[3] + arr[0] - arr[1] - arr[2]]\n",
    "    a = transform(a)\n",
    "    b = transform(b)\n",
    "    c = transform(c)\n",
    "    \n",
    "    qml.RZ(a[0], wires=wires[0])\n",
    "    qml.CRZ(a[1], wires=[wires[1], wires[0]])\n",
    "    qml.CRZ(a[2], wires=[wires[2], wires[0]])\n",
    "    qml.CRZ(a[3], wires=[wires[3], wires[0]])\n",
    "    \n",
    "    qml.RY(b[0], wires=wires[0])\n",
    "    qml.CRY(b[1], wires=[wires[1], wires[0]])\n",
    "    qml.CRY(b[2], wires=[wires[2], wires[0]])\n",
    "    qml.CRY(b[3], wires=[wires[3], wires[0]])\n",
    "    \n",
    "    qml.RZ(c[0], wires=wires[0])\n",
    "    qml.CRZ(c[1], wires=[wires[1], wires[0]])\n",
    "    qml.CRZ(c[2], wires=[wires[2], wires[0]])\n",
    "    qml.CRZ(c[3], wires=[wires[3], wires[0]])\n",
    "\n",
    "def apply_mixing_layer(params, wires):\n",
    "    qml.Rot(params[0], \n",
    "            params[1],\n",
    "            params[2],\n",
    "            wires=wires)\n",
    "\n",
    "@qml.qnode(dev2)\n",
    "def apply_layers_qram_train(params, wires, data, label):\n",
    "    #qram initialization on wires 1, 2; wire 3 is used as an auxilary\n",
    "    qml.Hadamard(wires=wires[1])\n",
    "    qml.Hadamard(wires=wires[2])\n",
    "    qml.Toffoli(wires=wires[1:])\n",
    "    \n",
    "    #reshaping parameters for ease\n",
    "    data_params = params[:,:6].reshape((len(params), 3, 2))\n",
    "    mixing_params = params[:,6:].reshape((len(params), 3))\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        apply_data_layer_qram(data_params[i], wires=wires, data=data)\n",
    "        apply_mixing_layer(mixing_params[i], wires=wires[0])\n",
    "    \n",
    "    #label encoding for batch fidelity\n",
    "    if label[1] == -1:\n",
    "        qml.CNOT(wires=[wires[1], wires[0]])\n",
    "    if label[2] == -1:\n",
    "        qml.CNOT(wires=[wires[2], wires[0]])\n",
    "    if label[3] == -1:\n",
    "        qml.CNOT(wires=[wires[3], wires[0]])\n",
    "    if label[0] == -1:\n",
    "        qml.PauliX(wires=wires[1])\n",
    "        qml.PauliX(wires=wires[2])\n",
    "        qml.Toffoli(wires=[wires[1], wires[2], wires[0]])\n",
    "    return qml.expval(qml.PauliZ(wires[0]))\n",
    "\n",
    "def cost_qram(w, X, Y):\n",
    "    x = apply_layers_qram_train(w, wires=[0, 1, 2, 3], data = X, label = Y)\n",
    "    return (1-x)/2\n",
    "\n",
    "n = int(len(data)/batch_size)\n",
    "n =1\n",
    "for a in range(n):\n",
    "    batch_index = np.random.randint(0, len(X_train), (batch_size,))\n",
    "    X_train_batch = []\n",
    "    Y_train_batch = []\n",
    "    for i in batch_index:\n",
    "        X_train_batch.append(X_train[i])\n",
    "        Y_train_batch.append(Y_train[i])\n",
    "    weights = opt.step(lambda w: cost_qram(w, X_train_batch, Y_train_batch), weights)\n",
    "    print(X_train_batch)\n",
    "    \n",
    "    start = time.time()\n",
    "    weights_2 = opt.step(lambda w: cost_qram(w, X_train_batch, Y_train_batch), weights)\n",
    "    x = cost_qram(weights, X_train_batch, Y_train_batch)\n",
    "    stop = time.time()\n",
    "    \n",
    "    print('Cost at step '+ str(a) + ': ' + str(x) + ' cal in ' + str(stop-start))\n",
    "    a = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    weights_2 = opt.step(lambda w: cost(w, X_train_batch, Y_train_batch), weights)\n",
    "    a = cost(weights, X_train_batch, Y_train_batch)\n",
    "    stop = time.time()\n",
    "    \n",
    "    print(str(a/4) + ' in ' + str(stop-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, for a simulator, the time of evaluation for the qrammed case is sometimes slower as compared to when each data point of the batch is individually evaluated. This would only get better as the number of qubits increase with an increase in batch size. Before proceding, lets do a short sanity check:  \n",
    "    For a n qubit qram, batch size is $2^n = N$.  \n",
    "        Ancilla qubits required for multi-qubit control gates $ = n-2$.  \n",
    "        Total qubits: $n + n-2 + 1 = 2n - 1$.  \n",
    "        Increase in circuit depth (due to qram) $\\rightarrow$ Increase in circuit depth of data layer (mixing layer remains the same and constant), thus by a factor of $\\approx N = 2^n$  \n",
    "        Number of measurements performed: 1 observable for $2^n$ test cases.\n",
    "    If we had instead just used single qubit classifier parallely on the $2n-1$ qubits available:\n",
    "        Time of evaluation: $O(\\frac{2^n}{2n-1}) \\approx O(2^{n-\\log n - 1})$  \n",
    "        Number of measurements performed: $2^n$ observables, $O(\\frac{2^n}{2n-1}) \\approx O(2^{n-\\log n - 1})$ number of experiments.  \n",
    "Lets now evaluate this on a real device and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum data re-uploading (?)  \n",
    "\n",
    "Due to no cloning, we cannot directly replicate quantum data and a naive attempt at data re-uploading boils down to something similar to normal quantum machine learning, albiet with quantum data, or also where classical input data amplitude encoded as any attempts to perform control gates controlled on the quantum data qubits are just similar entangling gates and the presence of input data in the quantum classifier is not  amplified, it's the same amount of information encoded. See figure below.  \n",
    "\n",
    "If the experiment to produce the input is reproducable, we propose a new method to learn quantum data and we test it out to see if we actually do better than a single input copy with an equivalent number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "To do logistics:\n",
    "References linking\n",
    "Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]]\n",
      "[[ 0.78240842  0.69264825  1.13852199  0.85281787  0.373434   -1.56332457\n",
      "   0.51637201  0.54697687 -0.09508682]\n",
      " [-1.11846523 -1.30291968  1.28845708  1.44962213 -1.1737583  -1.34303697\n",
      "  -0.29642611 -0.58089091  0.55810182]\n",
      " [ 0.3086709  -0.26668044  0.81010649 -0.47248028  0.18461197 -0.7331915\n",
      "  -0.59950573 -1.00760812 -1.2201845 ]\n",
      " [-0.54995691 -0.42004165 -1.42723698  0.22371955 -0.26179474 -0.22600968\n",
      "  -0.1253331   0.11654237  1.41791877]\n",
      " [-1.35635716 -1.18649491 -1.11280432 -0.34795293 -1.23064544 -0.59802982\n",
      "  -1.34095631  0.88590395 -1.28434254]]\n",
      "[[ 0.78240842  0.69264825  1.13852199  0.85281787]\n",
      " [-1.11846523 -1.30291968  1.28845708  1.44962213]\n",
      " [ 0.3086709  -0.26668044  0.81010649 -0.47248028]\n",
      " [-0.54995691 -0.42004165 -1.42723698  0.22371955]\n",
      " [-1.35635716 -1.18649491 -1.11280432 -0.34795293]]\n",
      "[[[ 0.78240842  0.69264825]\n",
      "  [ 1.13852199  0.85281787]\n",
      "  [ 0.373434   -1.56332457]]\n",
      "\n",
      " [[-1.11846523 -1.30291968]\n",
      "  [ 1.28845708  1.44962213]\n",
      "  [-1.1737583  -1.34303697]]\n",
      "\n",
      " [[ 0.3086709  -0.26668044]\n",
      "  [ 0.81010649 -0.47248028]\n",
      "  [ 0.18461197 -0.7331915 ]]\n",
      "\n",
      " [[-0.54995691 -0.42004165]\n",
      "  [-1.42723698  0.22371955]\n",
      "  [-0.26179474 -0.22600968]]\n",
      "\n",
      " [[-1.35635716 -1.18649491]\n",
      "  [-1.11280432 -0.34795293]\n",
      "  [-1.23064544 -0.59802982]]]\n"
     ]
    }
   ],
   "source": [
    "params = np.random.uniform(low=-np.pi / 2, high=np.pi / 2, size=(5, 9))\n",
    "p = np.zeros(shape = (5, 3, 2))\n",
    "print(p)\n",
    "print(params)\n",
    "print(params[:, :4])\n",
    "print(params[:, :6].reshape((len(params), 3, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "transform = lambda arr: [arr[0], arr[1] - arr[0], arr[2] - arr[0], arr[3] + arr[0] - arr[1] - arr[2]]\n",
    "print(transform([1, 2, 3, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References  \n",
    "[1] - https://quantum-journal.org/papers/q-2020-02-06-226/  \n",
    "[2] - https://github.com/AlbaCL/qhack21f  \n",
    "[3] - https://journals.aps.org/pra/abstract/10.1103/PhysRevA.102.032420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
